from wordfreq_builder.config import CONFIG, data_filename, wordlist_filename
import sys
import pathlib

HEADER = """# This file is automatically generated. Do not edit it.
# You can regenerate it using the 'wordfreq-build-deps' command.
"""
TMPDIR = data_filename('tmp')


# Set this to True to rebuild the Twitter tokenization (which takes days)
PRETOKENIZE_TWITTER = False


def make_ninja_deps(rules_filename, out=sys.stdout):
    """
    Output a complete Ninja file describing how to build the wordfreq data.
    """
    print(HEADER, file=out)
    # Copy in the rules section
    with open(rules_filename, encoding='utf-8') as rulesfile:
        print(rulesfile.read(), file=out)

    lines = []
    if PRETOKENIZE_TWITTER:
        lines.extend(
            twitter_preprocess_deps(
                data_filename('raw-input/twitter/all-2014.txt'),
                slice_prefix=data_filename('slices/twitter/tweets-2014'),
                combined_prefix=data_filename('intermediate/twitter/tweets-2014'),
                slices=40,
                languages=CONFIG['sources']['twitter']
            )
        )
    lines.extend(
        twitter_deps(
            data_filename('intermediate/twitter/tweets-2014'),
            languages=CONFIG['sources']['twitter']
        )
    )
    lines.extend(
        wikipedia_deps(
            data_filename('raw-input/wikipedia'),
            CONFIG['sources']['wikipedia']
        )
    )
    print('\n'.join(lines), file=out)


def wikipedia_deps(dirname_in, languages):
    lines = []
    path_in = pathlib.Path(dirname_in)
    for language in languages:
        # Find the most recent file for this language
        input_file = max(path_in.glob(
            '{}wiki*.bz2'.format(language)
        ))
        output_file = wordlist_filename('wikipedia', language, '')
        build_rule = "build {outs}: wiki2text {ins}".format(
            outs=output_file, ins=input_file
        )
        lines.append(build_rule)
        output_file = wordlist_filename('wikipedia', language, '.tokens')
        build_rule = "build {outs}: wiki2tokens {ins}".format(
            outs=output_file, ins=input_file
        )
        lines.append(build_rule)

        token_file = output_file
        output_file = wordlist_filename('wikipedia', language, '.counts')
        build_rule = "build {outs}: count {ins}".format(
            outs=output_file, ins=token_file
        )
        lines.append(build_rule)
        lines.append("  tmp = {}".format(TMPDIR))
    return lines


def twitter_preprocess_deps(input_filename, slice_prefix,
                            combined_prefix, slices, languages):
    lines = []

    slice_files = ['{prefix}.part{num:0>2d}'.format(prefix=slice_prefix, num=num)
                   for num in range(slices)]
    # split the input into slices
    build_rule = "build {outs}: split {ins}".format(
        outs=' '.join(slice_files), ins=input_filename
    )
    lines.append(build_rule)
    lines.append("  prefix = {}.part".format(slice_prefix))
    lines.append("  slices = {}".format(slices))
    lines.append("")

    for slicenum in range(slices):
        slice_file = slice_files[slicenum]
        language_outputs = [
            '{prefix}.{lang}.txt'.format(prefix=slice_file, lang=language)
            for language in languages
        ]
        build_rule = "build {outs}: tokenize_twitter {ins}".format(
            outs=' '.join(language_outputs), ins=slice_file
        )
        lines.append(build_rule)
        lines.append("  prefix = {}".format(slice_file))
        lines.append("")

    for language in languages:
        combined_output = '{prefix}.{lang}.txt'.format(prefix=combined_prefix, lang=language)

        language_inputs = [
            '{prefix}.{lang}.txt'.format(prefix=slice_files[slicenum], lang=language)
            for slicenum in range(slices)
        ]
        build_rule = "build {outs}: cat {ins}".format(
            outs=combined_output,
            ins=' '.join(language_inputs)
        )
        lines.append(build_rule)


def twitter_deps(prefix_in, languages):
    lines = []
    for language in languages:
        input_file = '{prefix}.{lang}.txt'.format(prefix=prefix_in, lang=language)
        output_file = wordlist_filename('twitter', language, '.tokens')
        build_rule = "build {outs}: format_twitter {ins} | {deps}".format(
            outs=output_file,
            ins=input_file,
            deps='wordfreq_builder/tokenizers.py'
        )
        lines.append(build_rule)

        token_file = output_file
        output_file = wordlist_filename('twitter', language, '.counts')
        build_rule = "build {outs}: count {ins}".format(
            outs=output_file, ins=token_file
        )
        lines.append(build_rule)
        lines.append("  tmp = {}".format(TMPDIR))

    return lines


def main():
    make_ninja_deps('rules.ninja')


if __name__ == '__main__':
    main()
