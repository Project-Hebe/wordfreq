from wordfreq_builder.config import CONFIG, data_filename
import sys
import pathlib

HEADER = """# This file is automatically generated. Do not edit it.
# You can regenerate it using the 'wordfreq-build-deps' command.
"""


def make_ninja_deps(rules_filename, out=sys.stdout):
    """
    Output a complete Ninja file describing how to build the wordfreq data.
    """
    print(HEADER, file=out)
    # Copy in the rules section
    with open(rules_filename, encoding='utf-8') as rulesfile:
        print(rulesfile.read(), file=out)

    lines = (
        language_detect_and_tokenize_deps(
            data_filename('raw-input/twitter/all-2014.txt'),
            slice_prefix=data_filename('slices/twitter/tweets-2014'),
            combined_prefix=data_filename('generated/twitter/tweets-2014'),
            slices=40
        ) +
        wiki_parse_deps(
            data_filename('raw-input/wikipedia'),
            data_filename('generated/wikipedia'),
            CONFIG['wp_languages']
        )
    )
    print('\n'.join(lines), file=out)


def wiki_parse_deps(dirname_in, dirname_out, languages):
    lines = []
    path_in = pathlib.Path(dirname_in)
    path_out = pathlib.Path(dirname_out)
    for language in languages:
        # Find the most recent file for this language
        input_file = max(path_in.glob(
            '{}wiki*.bz2'.format(language)
        ))
        output_file = path_out / 'wikipedia_{}.txt'.format(language)
        build_rule = "build {outs}: wiki2text {ins}".format(
            outs=output_file, ins=input_file
        )
        lines.append(build_rule)
        output_file = path_out / 'wikipedia_{}.tokens.txt'.format(language)
        build_rule = "build {outs}: wiki2tokens {ins}".format(
            outs=output_file, ins=input_file
        )
        lines.append(build_rule)
    return lines


def language_detect_and_tokenize_deps(input_filename, slice_prefix,
                                      combined_prefix, slices):
    lines = []
    # split the input into slices
    slice_files = ['{prefix}.part{num:0>2d}'.format(prefix=slice_prefix, num=num)
                   for num in range(slices)]
    build_rule = "build {outs}: split {ins}".format(
        outs=' '.join(slice_files), ins=input_filename
    )
    lines.append(build_rule)
    lines.append("  prefix = {}.part".format(slice_prefix))
    lines.append("  slices = {}".format(slices))
    lines.append("")

    for slicenum in range(slices):
        slice_file = slice_files[slicenum]
        language_outputs = [
            '{prefix}.{lang}.txt'.format(prefix=slice_file, lang=language)
            for language in CONFIG['languages']
        ]
        build_rule = "build {outs}: tokenize_twitter {ins}".format(
            outs=' '.join(language_outputs), ins=slice_file
        )
        lines.append(build_rule)
        lines.append("  prefix = {}".format(slice_file))
        lines.append("")

    for language in CONFIG['languages']:
        combined_output = '{prefix}.{lang}.txt'.format(prefix=combined_prefix, lang=language)
        language_inputs = [
            '{prefix}.{lang}.txt'.format(prefix=slice_files[slicenum], lang=language)
            for slicenum in range(slices)
        ]
        build_rule = "build {outs}: cat {ins}".format(
            outs=combined_output,
            ins=' '.join(language_inputs)
        )
        lines.append(build_rule)

    return lines


def main():
    make_ninja_deps('rules.ninja')


if __name__ == '__main__':
    main()
